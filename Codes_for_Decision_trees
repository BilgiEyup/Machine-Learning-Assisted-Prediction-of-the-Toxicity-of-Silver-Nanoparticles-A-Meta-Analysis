#Make sure to import the following packages to perform the analyses in the article.


!pip install dtreeviz #For decision tree visualization
import dtreeviz
import sys
import os
import dtreeviz.trees
import pandas as pd
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt 

#The packages below are used for decision trees
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split 
from sklearn import metrics
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn import tree
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    confusion_matrix,
    accuracy_score,
    precision_score,
    recall_score,
    f1_score
)
from sklearn.utils import shuffle

#The packages below are used for classifiers other than decision trees.

from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import train_test_split

#to read and assign dataset to a dataframe
df = pd.read_csv('/path_of_your_dataset/dataset_name.csv',sep=',',decimal='.')

#to see column names
df.columns

#to see dataframe's head and tail
df

#To assign a new dataframe that only contains the requested columns, use the following code:
df2=df[['column_name1','column_name2','column_name3','column_name4','column_name7',]]



#"In this article, the bioactivity threshold value is determined as 50. 
#Therefore, those with cytotoxicity values of 50 or less are labeled as toxic, 
#and those above 50 are labeled as non-toxic. By updating the following code 
#according to your own threshold values, you can add these categories to a new column.

#For example, you can consider the values between 90% and 110% as non-toxic, 
#above 110% as proliferative, and between 50% and 90% as intermediate toxicity. 
#You can add these values to a list called "bioactivity_threshold" and 
#then append it to the "toxicology_class" column.

#numeric_column_name is Cell_viability for our raw dataset
bioactivity_threshold = []
for i in df2.numeric_column_name:        
  if float(i) >= 90 and float(i)<= 110:
    bioactivity_threshold.append("nontoxic")
  if float(i)>50 and float(i)<90:
    bioactivity_threshold.append('nontoxic')
  elif float(i) <= 50:
    bioactivity_threshold.append("toxic")
  elif float(i)>110:
    bioactivity_threshold.append("nontoxic")
bioactivity_class = pd.Series(bioactivity_threshold, name='toxicology_class')
df3 = pd.concat([df2, bioactivity_class], axis=1)
df4=df3[df3.toxicology_class.notna()]
df5=df4[df4.Cell_viability.notna()]
df5

#Since categorical and binary data are more useful for classification algorithms, 
#we can create a new column called 'toxicology_category' using the data in the 
#'toxicology_class' column we previously created and 
#add it to a new dataframe called df6 using the following code:

class_cat = []
for i in df5.toxicology_class:        
  if i=='toxic':
    class_cat.append("0")
  if i=='nontoxic':
    class_cat.append("1")

class_category = pd.Series(class_cat, name='toxicology_category')
df6 = pd.concat([df5, class_category], axis=1)
df6

#As an alternative, you can use this code
df6 = pd.get_dummies(df5, drop_first=True) # to create categorical versions of all columns including toxicology_class

#to drop unwanted raws you can use the code below:

df5.drop(df5.loc[df5['Concentration_category']==1].index, inplace=True) #in this article concentration divided into 9 different category, not to create bias from untreated cells we preferred to drop those raws.

#If you have numeric rows with NA values, you can discard them 
#by writing the relevant column name in the following code.
df6 = df5[df5.colum_name.notna()]


#Since toxicology_class_toxic will be the endpoint we used in the article, we can separate it from the dataframe and 
#assign it to a different variable for later use in classification.
encoded_x = df6.drop('toxicology_class_toxic', axis=1)
encoded_y=df6['toxicology_class_toxic']

#to see distributions in dataframe:

item_counts = df6["toxicology_class_toxic"].value_counts()
item_counts


#AFTER COMPLETING THE PREPARATION PROCESS, 
#WE CAN START THE ANALYSIS ON THE DATAFRAME WITH THE FOLLOWING CODES

shuffled = shuffle(df6) #To shuffle the dataframe in a way that does not create bias:
feature_cols = ['Feautre_column1','Feature_column2', 'Feature_column3', 'Feature_column4']
X = shuffled[feature_cols] # Features

y = shuffled.toxicology_class_toxic # Target variable
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 12345, stratify = y) 
# 70% training and 30% test, 
#stratify=y ensures that the target binary variable (such as toxic=0, non-toxic=1) is distributed 
#in the test and train sets in the same proportion as it is distributed 
#throughout the entire dataframestratify=y ensures random split of data 

# Create Decision Tree classifer object
clf = DecisionTreeClassifier(max_depth=5, min_samples_leaf=15) #you can change the depth and leaf number 

# Train Decision Tree Classifer
clf = clf.fit(X_train,y_train)
y_pred = clf.predict(X_test)
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))

#to get metrics 
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1score = f1_score(y_test, y_pred)
accuracy = accuracy_score(y_test, y_pred)
print('Precision: ',precision)
print('Recall: ', recall)
print('f1score: ', f1score)

shuffled.to_csv('<path_to_computer>/shuffled_dataset.csv')

#TO CREATE PLOT THAT SHOWS FEATURE IMPORTANCE WITH VALUES

# Create plot
fig, ax = plt.subplots()

# Create plot title
ax.set_title("Feature Importance")

# Add bars
bars = ax.bar(range(X.shape[1]), importances[indices])

# Add feature names as x-axis labels
ax.set_xticks(range(X.shape[1]))
ax.set_xticklabels(names, rotation=90)

# Show values on bar
for bar in bars:
    height = bar.get_height()
    ax.text(bar.get_x() + bar.get_width() / 2, height, round(height, 2), ha="center", va="bottom")

plt.savefig('featureDT1_valued.svg', format='svg')
# See plot
plt.show()

#TO SEE WHETHER THE INCLUDED FEATURE USED IN THE MODEL USE THE CODE BELOW

# define the feature columns and target variable
feature_cols = ['Feature1', 'Feature2', 'Feature3', 'Feature4']
X = shuffled[feature_cols]
y = shuffled['target_variable']

# split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12345, stratify=y)

# train the decision tree classifier
clf = DecisionTreeClassifier(max_depth=5, min_samples_leaf=15, random_state=0)
clf.fit(X_train, y_train)

# get the features used in the decision tree
used_features = set()
for feature, threshold, _, _ in zip(clf.tree_.feature, clf.tree_.threshold, clf.tree_.children_left, clf.tree_.children_right):
    if feature >= 0:
        used_features.add(feature_cols[feature])

print("Features used in the decision tree:")
print(used_features)


#TO GET AREA UNDER CURVES FOR DIFFERENT CLASSIFIER

# Instantiate the classfiers and make a list
classifiers = [LogisticRegression(random_state=1234), 
               GaussianNB(), 
               KNeighborsClassifier(), 
               DecisionTreeClassifier(random_state=1234),
               RandomForestClassifier(random_state=1234)]

# Define a result table as a DataFrame
result_table = pd.DataFrame(columns=['classifiers', 'fpr','tpr','auc'])

# Train the models and record the results
for cls in classifiers:
    model = cls.fit(X_train, y_train)
    yproba = model.predict_proba(X_test)[::,1]
    
    fpr, tpr, _ = roc_curve(y_test,  yproba)
    auc = roc_auc_score(y_test, yproba)
    
    result_table = result_table.append({'classifiers':cls.__class__.__name__,
                                        'fpr':fpr, 
                                        'tpr':tpr, 
                                        'auc':auc}, ignore_index=True)

# Set name of the classifiers as index labels
result_table.set_index('classifiers', inplace=True)

#TO VISUALIZE THE ROC CURVE

fig = plt.figure(figsize=(8,6))

for i in result_table.index:
    plt.plot(result_table.loc[i]['fpr'], 
             result_table.loc[i]['tpr'], 
             label="{}, AUC={:.3f}".format(i, result_table.loc[i]['auc']))
    
plt.plot([0,1], [0,1], color='orange', linestyle='--')

plt.xticks(np.arange(0.0, 1.1, step=0.1))
plt.xlabel("Flase Positive Rate", fontsize=15)

plt.yticks(np.arange(0.0, 1.1, step=0.1))
plt.ylabel("True Positive Rate", fontsize=15)

plt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)
plt.legend(prop={'size':13}, loc='lower right')

plt.savefig('roc_curve.svg', format='svg')
plt.show()


#TO COMPARE CLASSIFIERS
# Define feature columns and target variable
feature_cols = ['Feature1', 'Feature2', 'Feature3', 'Feature4']
target_col = 'target_variable'

# Split data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12345, stratify=y)

# Define models
models = [
    ('Logistic Regression', LogisticRegression()),
    ('Gaussian Naive Bayes', GaussianNB()),
    ('K-Nearest Neighbors', KNeighborsClassifier()),
    ('Random Forest', RandomForestClassifier()),
    ("Decision Tree", DecisionTreeClassifier(max_depth=5, min_samples_leaf=15))
]

# Evaluate models
results = []
for name, model in models:
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = round(accuracy_score(y_test, y_pred), 2)
    precision = round(precision_score(y_test, y_pred), 2)
    recall = round(recall_score(y_test, y_pred), 2)
    f1score = round(f1_score(y_test, y_pred), 2)
    results.append((name, accuracy, precision, recall, f1score))

# Convert results to dataframe
results_df = pd.DataFrame(results, columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])

# Display results as table
print(results_df)

# Save table as image
fig, ax = plt.subplots(figsize=(10, 5))
ax.axis('off')
ax.axis('tight')
ax.table(cellText=results_df.values,colLabels=results_df.columns, loc='center')
plt.savefig('path_to_computer/Comparison.svg', dpi=900, bbox_inches='tight')

#TO VISUALIZE DECISION TREE

%config InlineBackend.figure_format = 'retina' # Make visualizations look good
%config InlineBackend.figure_format = 'svg' 
%matplotlib inline

if 'google.colab' in sys.modules:
  !pip install -q dtreeviz
import pandas as pd
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor

import dtreeviz
dataset=shuffled
random_state = 12345 
stratify=y # get reproducible trees
features = ['Concentration','Size_nm', 'Exposure_time_h', 'Coat_categorical']
target = "toxicology_class_toxic"

tree_classifier = DecisionTreeClassifier(max_depth=4, random_state=random_state)
tree_classifier.fit(dataset[features].values, dataset[target].values)
viz_model = dtreeviz.model(tree_classifier,
                           X_train=dataset[features], y_train=dataset[target],
                           feature_names=features,
                           target_name=target,class_names=["nontoxic", "toxic"])
viz_model.rtree_feature_space3D
viz_model.view(fancy=True)
#viz_model.view(fancy=False) #you can activate if you want a more detailed visualization

v = viz_model.view()     # render as SVG into internal object 
v.show()                 # pop up window
v.save("/path_to_your_computer/decisiontree.svg")  # optionally save as svg
